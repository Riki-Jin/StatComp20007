}))
power_l_f <- mean(replicate(m, expr={
x <- rnorm(n_large, 0, sigma1)
y <- rnorm(n_large, 0, sigma2)
as.integer(var.test(x,y)$p.value < 0.055)
}))
c(power_s_c5, power_s_f)
# For small sample sizes, Count Five Test power=0.3115, F-Test power=0.4220
c(power_m_c5, power_m_f)
# For medium sample sizes, Count Five Test power=0.8060, F-Test power=0.9500
c(power_l_c5, power_l_f)
# For large sample sizes, Count Five Test power=0.9550, F-Test power=1
alpha <- .1
n <- 30
m <- 30    # It should be 100, but to shorten the vignettes creation time, I manually switch it to 30.
epsilon <- c(seq(0, .15, .05), seq(.2, 1, .2))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qchisq(0.975,d*(d+1)*(d+2)/6)
for (j in 1:N) {    # for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) {    # for each replicate
x <- (1-e)*mvrnorm(n, c(0,0), matrix(c(1,0,0,1),2,2)) + e*mvrnorm(n, c(0,0), matrix(c(100,0,0,100),2,2))
sktests[i] <- as.integer(n*abs(mul_sk(x))/6 >= cv)
}
pwr[j] <- mean(sktests)
}
# plot power vs epsilon
plot(epsilon, pwr, type = "b", xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m)    # add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
# This is a repetition of Example 6.10 for multivariate case.
set.seed(3333)
library(MASS)
mul_sk <- function(x) {
# computes the Mardia multivariate sample skewness coeff.
n <- nrow(x)
sigma <- var(x)
sigmainv <- solve(sigma)
mu <- apply(x,2,mean)
x_cen <- x-mu
sk <- 0
for (i in 1:n)
for (j in 1:n)
sk <- sk + (t(as.matrix(x_cen[i,])) %*% sigmainv %*% as.matrix((x_cen[j,])))^3
sk/n^2
}
d <- 2    # We suppose the dimension is 2
n <- c(5, 10, 20, 30)    # sample sizes
cv <- qchisq(0.975,d*(d+1)*(d+2)/6)
# n is a vector of sample sizes
# we are doing length(n) different simulations
p.reject <- numeric(length(n))    # to store sim. results
m <- 100    # num. repl. each sim.  It should be 1000, but to shorten the vignettes creation time, I manually switch it to 100.
for (i in 1:length(n)) {
sktests <- numeric(m)    # test decisions
for (j in 1:m) {
x <- mvrnorm(n[i],c(0,0),matrix(c(1,0,0,1),2,2))    # test decision is 1 (reject) or 0
sktests[j] <- as.integer(n[i]*abs(mul_sk(x))/6 >= cv)
}
p.reject[i] <- mean(sktests)    # proportion rejected
}
p.reject
# when n=5,10,20,30, p.reject=0.222,0.146,0.114,0.107
# This is a repetition of Example 6.8 for multivariate case.
alpha <- .1
n <- 30
m <- 30    # It should be 100, but to shorten the vignettes creation time, I manually switch it to 30.
epsilon <- c(seq(0, .15, .05), seq(.2, 1, .2))
N <- length(epsilon)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qchisq(0.975,d*(d+1)*(d+2)/6)
for (j in 1:N) {    # for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
for (i in 1:m) {    # for each replicate
x <- (1-e)*mvrnorm(n, c(0,0), matrix(c(1,0,0,1),2,2)) + e*mvrnorm(n, c(0,0), matrix(c(100,0,0,100),2,2))
sktests[i] <- as.integer(n*abs(mul_sk(x))/6 >= cv)
}
pwr[j] <- mean(sktests)
}
# plot power vs epsilon
plot(epsilon, pwr, type = "b", xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m)    # add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
# This is a repetition of Example 6.10 for multivariate case.
set.seed(3333)
library(bootstrap)
law <- as.matrix(law)
corr <- cor(law[,1],law[,2])
n <- nrow(law)
jackknife_store <- rep(0,n)
for (i in 1:n) {
jacksample <- law[-i,]
jackknife_store[i] <- cor(jacksample[,1],jacksample[,2])
}
jackknife_bias <- (n-1)*(mean(jackknife_store)-corr)
jackknife_se <- (n-1)*sd(jackknife_store)/sqrt(n)
c(jackknife_bias, jackknife_se)
# Bias=-0.006473623, SE=0.1425186
set.seed(3333)
library(boot)
aircondit <- as.matrix(aircondit)
boot.mean <- function(x,i) mean(x[i])
boot.obj <- boot(aircondit, statistic=boot.mean, R=2000)
print(boot.ci(boot.obj, type = c("norm","basic","perc","bca")))
set.seed(3333)
library(bootstrap)
scor <- as.matrix(scor)
n <- nrow(scor)
lambda <- eigen(cov(scor))$values
theta <- lambda[1] / (lambda[1]+lambda[2]+lambda[3]+lambda[4]+lambda[5])
jackknife_theta <- rep(0,n)
for (i in 1:n) {
jacksample <- scor[-i,]
lambda_j <- eigen(cov(jacksample))$values
jackknife_theta[i] <- lambda_j[1] / (lambda_j[1]+lambda_j[2]+lambda_j[3]+lambda_j[4]+lambda_j[5])
}
jackknife_bias <- (n-1)*(mean(jackknife_theta)-theta)
jackknife_se <- (n-1)*sd(jackknife_theta)/sqrt(n)
c(jackknife_bias,jackknife_se)
# Bias=0.001069139, SE=0.049552307
library(DAAG)
attach(ironslag)
n <- length(magnetic)
e1 <- numeric(n*(n-1)/2)
e2 <- numeric(n*(n-1)/2)
e3 <- numeric(n*(n-1)/2)
e4 <- numeric(n*(n-1)/2)
count <- 0
for (i in 1:(n-1))
for (j in (i+1):n) {
count <- count+1
y <- magnetic[-c(i,j)]
x <- chemical[-c(i,j)]
P1 <- lm(y~x)
y1_1 <- chemical[i]*P1$coef[2] + P1$coef[1]
y1_2 <- chemical[j]*P1$coef[2] + P1$coef[1]
e1[count] <- (magnetic[i]-y1_1)^2+(magnetic[j]-y1_2)^2
P2 <- lm(y~x+I(x^2))
y2_1 <- P2$coef[1] + P2$coef[2] * chemical[i] + P2$coef[3] * chemical[i]^2
y2_2 <- P2$coef[1] + P2$coef[2] * chemical[j] + P2$coef[3] * chemical[j]^2
e2[count] <- (magnetic[i]-y2_1)^2+(magnetic[j]-y2_2)^2
P3 <- lm(log(y)~x)
y3_1 <- exp(P3$coef[1] + P3$coef[2] * chemical[i])
y3_2 <- exp(P3$coef[1] + P3$coef[2] * chemical[j])
e3[count] <- (magnetic[i]-y3_1)^2+(magnetic[j]-y3_2)^2
P4 <- lm(log(y)~log(x))
y4_1 <- exp(P4$coef[1] + P4$coef[2] * log(chemical[i]))
y4_2 <- exp(P4$coef[1] + P4$coef[2] * log(chemical[j]))
e4[count] <- (magnetic[i]-y4_1)^2+(magnetic[j]-y4_2)^2
}
c(mean(e1)/2,mean(e2)/2,mean(e3)/2,mean(e4)/2)
# 19.57227 17.87018 18.45491 20.46718
# According to the prediction error criterion, Model 2, the quadratic model, would again be the best fit for the data.
# Case without Permutation:
set.seed(123)
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
return(as.integer(max(c(outx, outy)) > 5))
}
n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 1000
alphahat <- mean(replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
count5test(x, y)
}))
print(alphahat)
# 0.104, suggests that the “Count Five” criterion does not necessarily control Type I error at alpha <= 0.0625 when the sample sizes are unequal.
# Now we use Permutation Method to deal with the same data:
per_countfivetest <- function(z,B) {
n <- length(z)
rst <- rep(0,B)
for (i in 1:B) {
z <- sample(z)
rst[i] <- count5test(z[1:(n/2)],z[-(1:(n/2))])
}
mean(rst)
}
alphahat2 <- mean(replicate(m, expr={
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
x <- x - mean(x) #centered by sample mean
y <- y - mean(y)
per_countfivetest(c(x,y),50)
}))
print(alphahat2)
# 0.06154, this is a good Type-I error, which is much closer to 0.0625, and suggests that this is an available permutation test for equal variance that applies when sample sizes are not necessarily equal.
set.seed(333)
library(Ball)
library(energy)
library(boot)
library(RANN)
m <- 100
k <- 3
p <- 2
mu <- 0.2
R <- 99
n <- n1+n2
N <- c(n1,n2)
alpha <- 0.1
sample_size <- c(10,20,30,50,80,100)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) # what's the first column?
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
compare_1 <- function(sample_size) {
n1 <- n2 <- sample_size
n <- n1+n2
N <- c(n1,n2)
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- matrix(rnorm(n1*p,0,1.7),ncol=p);
y <- cbind(rnorm(n2,0,1.7),rnorm(n2));
z <- rbind(x,y)
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,num.permutations=99,seed=i)$p.value
}
colMeans(p.values<alpha)
}
case1 <- matrix(0,nrow=6,ncol=3)
for (i in 1:6)
case1[i,] <- compare_1(sample_size[i])
plot(x=sample_size, y=case1[,1], type='b', xlab='Sample size', ylab='Power', main='Unequal variances and equal expectations',col=1,lwd=2,xlim=c(5,105),ylim=c(0,1))
lines(x=sample_size, y=case1[,2], type="b", lwd=2, col=2)
lines(x=sample_size, y=case1[,3], type="b", lwd=2, col=3)
legend(x=5,y=1,legend=c('NN','Energy','Ball'),lwd=2,col=c(1,2,3))
set.seed(3333)
rw.Metro.Laplace <- function(sigma,x0,N) {
u <- runif(N)
x <- rep(0,N)
x[1] <- x0
k <- 0
for (i in 2:N) {
y <- rnorm(1, mean=x[i-1], sd=sigma)
if (u[i] <= exp(abs(x[i-1])-abs(y)))
x[i] <- y else {
x[i] <- x[i-1]
k <- k+1
}
}
return(list(x=x,k=k))
}
sigma <- c(0.05,0.2,0.5,1,2,10)
x0 <- 20
N <- 2000
rw1 <- rw.Metro.Laplace(sigma[1],x0,N)
rw2 <- rw.Metro.Laplace(sigma[2],x0,N)
rw3 <- rw.Metro.Laplace(sigma[3],x0,N)
rw4 <- rw.Metro.Laplace(sigma[4],x0,N)
rw5 <- rw.Metro.Laplace(sigma[5],x0,N)
rw6 <- rw.Metro.Laplace(sigma[6],x0,N)
print(cbind(sigma=sigma, acc_rate=1-c(rw1$k,rw2$k,rw3$k,rw4$k,rw5$k,rw6$k)/N))
par(mfrow=c(2,3))
plot(rw1$x,type='l',xlab='sigma=0.05',ylab='X')
plot(rw2$x,type='l',xlab='sigma=0.2',ylab='X')
plot(rw3$x,type='l',xlab='sigma=0.5',ylab='X')
plot(rw4$x,type='l',xlab='sigma=1',ylab='X')
plot(rw5$x,type='l',xlab='sigma=2',ylab='X')
plot(rw6$x,type='l',xlab='sigma=10',ylab='X')
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}
k <- 4    # four chains
x0 <- c(-10,-5,5,10)    # overdispersed initial values
N <- 10000    # length of chains
b <- 200    # burn-in length
par(mfrow=c(2,2))
X <- matrix(nrow=k,ncol=N)
for (i in 1:k)
X[i,] <- rw.Metro.Laplace(0.2,x0[i],N)$x
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, N)
for (j in (1000+1):N)
rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(1000+1):N], type="l", xlab="sigma=0.2", ylab="R_hat")
abline(h=1.2, lty=2)
X <- matrix(nrow=k,ncol=N)
for (i in 1:k)
X[i,] <- rw.Metro.Laplace(1,x0[i],N)$x
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, N)
for (j in (500+1):N)
rhat[j] <- Gelman.Rubin(psi[,1:j])
x2 <- min(which(rhat>0 & rhat<1.2))
plot(rhat[(500+1):N], type="l", xlab="sigma=1", ylab="R_hat")
abline(h=1.2, lty=2)
X <- matrix(nrow=k,ncol=N)
for (i in 1:k)
X[i,] <- rw.Metro.Laplace(4,x0[i],N)$x
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, N)
for (j in (b+1):N)
rhat[j] <- Gelman.Rubin(psi[,1:j])
x3 <- min(which(rhat>0 & rhat<1.2))
plot(rhat[(b+1):N], type="l", xlab="sigma=4", ylab="R_hat")
abline(h=1.2, lty=2)
X <- matrix(nrow=k,ncol=N)
for (i in 1:k)
X[i,] <- rw.Metro.Laplace(16,x0[i],N)$x
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
psi[i,] <- psi[i,] / (1:ncol(psi))
rhat <- rep(0, N)
for (j in (b+1):N)
rhat[j] <- Gelman.Rubin(psi[,1:j])
x4 <- min(which(rhat>0 & rhat<1.2))
plot(rhat[(b+1):N], type="l", xlab="sigma=16", ylab="R_hat")
abline(h=1.2, lty=2)
c(x2,x3,x4)
devtools::document()
devtools::build_vignettes()
library(StatComp20007)
detach("package:StatComp20007", unload = TRUE)
devtools::install_github("Riki-Jin/StatComp20007",
build_vignettes = TRUE, force=T)
library(StatComp20007)
S <- function(k,a) {
tmp <- sqrt(a^2*k/(k+1-a^2))
pt(tmp, df=k)
}
k_store <- 4:25
result <- numeric(length(k_store)+3)
count <- 0
for (k in k_store) {
count <- count+1
target <- function(a) {
S(k,a)-S(k-1,a)
}
out <- uniroot(target,  lower = 0.001, upper = sqrt(k)-0.1)
result[count] <- out$root
}
target_100 <- function(a) {
S(100,a)-S(99,a)
}
out <- uniroot(target_100,  lower = 0.001, upper = sqrt(100)-4)    # To avoid meeting upper bound
result[23] <- out$root
target_500 <- function(a) {
S(500,a)-S(499,a)
}
out <- uniroot(target_500,  lower = 0.001, upper = sqrt(500)-20)    # To avoid meeting upper bound
result[24] <- out$root
target_1000 <- function(a) {
S(1000,a)-S(999,a)
}
out <- uniroot(target_1000,  lower = 0.001, upper = sqrt(1000)-25)    # To avoid meeting upper bound
result[25] <- out$root
cbind(k=c(k_store,100,500,1000),result)
set.seed(3333)
rw.Metro.Laplace <- function(sigma,x0,N) {
u <- runif(N)
x <- rep(0,N)
x[1] <- x0
k <- 0
for (i in 2:N) {
y <- rnorm(1, mean=x[i-1], sd=sigma)
if (u[i] <= exp(abs(x[i-1])-abs(y)))
x[i] <- y else {
x[i] <- x[i-1]
k <- k+1
}
}
return(list(x=x,k=k))
}
sigma <- c(0.05,0.2,0.5,1,2,10)
x0 <- 20
N <- 2000
rw1 <- rw.Metro.Laplace(sigma[1],x0,N)
rw2 <- rw.Metro.Laplace(sigma[2],x0,N)
rw3 <- rw.Metro.Laplace(sigma[3],x0,N)
rw4 <- rw.Metro.Laplace(sigma[4],x0,N)
rw5 <- rw.Metro.Laplace(sigma[5],x0,N)
rw6 <- rw.Metro.Laplace(sigma[6],x0,N)
print(cbind(sigma=sigma, acc_rate=1-c(rw1$k,rw2$k,rw3$k,rw4$k,rw5$k,rw6$k)/N))
par(mfrow=c(2,3))
plot(rw1$x,type='l',xlab='sigma=0.05',ylab='X')
plot(rw2$x,type='l',xlab='sigma=0.2',ylab='X')
plot(rw3$x,type='l',xlab='sigma=0.5',ylab='X')
plot(rw4$x,type='l',xlab='sigma=1',ylab='X')
plot(rw5$x,type='l',xlab='sigma=2',ylab='X')
plot(rw6$x,type='l',xlab='sigma=10',ylab='X')
devtools::install_github("Riki-Jin/StatComp20007",build_vignettes = TRUE,force=T)
remove.packages("StatComp20007")
devtools::install_github("Riki-Jin/StatComp20007",build_vignettes = TRUE,force=T)
library(StatComp20007)
sigma <- 2
x0 <- 5
N <- 2000
rwC(sigma,x0,N)
detach("package:StatComp20007", unload = TRUE)
remove.packages("StatComp20007")
install.packages("C:/Users/Zhang Jin/Desktop/Statistical Computing/StatComp20007_1.0.tar.gz",repo=NULL)
rwC(2,5,2000)
?mvrnorm
library(MASS)
x <- mvrnorm(10000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
mardias <- function(x, na.rm = TRUE){
if (na.rm)
x <- na.omit(x)
n <- dim(x)[1]
p <- dim(x)[2]
x <- scale(x, scale = FALSE)
S <- cov(x)*(n-1)/n
S.inv <- MASS::ginv(S)
D <- x %*% S.inv %*% t(x)
b1p <- sum(D^3)/n^2
b2p <- sum(diag(D^2))/n
return(c(b1p,b2p))
}
library(MASS)
x <- mvrnorm(10000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
set.seed(1)
library(MASS)
x <- mvrnorm(10000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
set.seed(2)
library(MASS)
x <- mvrnorm(10000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
set.seed(1)
library(MASS)
x <- mvrnorm(1000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
set.seed(3)
library(MASS)
x <- mvrnorm(1000,c(1,2,3),matrix(c(4,0.7,0.3,0.7,2,1.4,0.3,1.4,5),nrow=3,ncol=3))
mardias(x)
devtools::document()
devtools::document()
rm(list = c("mardias"))
devtools::document()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::build_vignettes()
devtools::check()
n <- 10000
a <- 2
b <- 2
unifrv <- runif(n)
paretorv <- b/(1-unifrv)^(1/a)    # inverse transformation
hist(paretorv[paretorv>0 & paretorv<20], freq = FALSE, breaks = seq(0,20,0.5), main = "Histogram of the Pareto sample with the true density curve",xlab = "Sample value")    # graph the density histogram
# I found that F(20)=0.99, which is very close to 1. For the sake of the tidiness and better description for the feature of the histogram, I made a truncation on x=20, and only consider the variables between 0 and 20.
f <- function(x) {a*b^a/x^(a+1)}    # true pdf
curve(f, 2, 20, col = 2, lwd = 3, add = TRUE)    # add the true density curve
legend(12,0.6,"true density", col = 2, lwd = 3)    # add a legend
devtools::document()
devtools::build_vignettes()
devtools::check()
devtools::build()
remove.packages("StatComp20007")
install.packages("C:/Users/Zhang Jin/Desktop/Statistical Computing/StatComp20007_1.0.tar.gz",repo=NULL)
install.packages("C:/Users/Zhang Jin/Desktop/Statistical Computing/StatComp20007_1.0.tar.gz",repo=NULL)
library(StatComp20007)
rwC(1,2)
mardias(matrix(c(1,2,3,4,5,6,7,8),2,4))
mardias(matrix(c(1,2,3,4,5,6,7,8),4,2))
devtools::document()
devtools::build_vignettes()
devtools::check()
library(StatComp20007)
detach("package:StatComp20007", unload = TRUE)
devtools::build()
remove.packages("StatComp20007")
